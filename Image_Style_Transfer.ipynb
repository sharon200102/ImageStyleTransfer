{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-Style-Transfer",
      "provenance": [],
      "authorship_tag": "ABX9TyN5JzvtmnZT1peCXmT8wW9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon200102/ImageStyleTransfer/blob/main/Image_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic initialization\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.utils import save_image\n",
        "# enable cuda device\n",
        "GPU = 0\n",
        "epoch = 0\n",
        "content_img_path = Path('/content/Neckarfront_TÃ¼bingen_Mai_2017.jpg')\n",
        "style_img_path = Path('/content/starry-night-1093721_960_720.jpg')\n",
        "writer = SummaryWriter('/content/imag_style_transfer_exp')\n",
        "pre_trained_model_name = 'vgg19'\n",
        "patience = 100\n",
        "alpha =1\n",
        "beta=1000\n",
        "content_representation_layer = 21\n",
        "style_representation_layres = [0,5,10,19,28]\n",
        "weights = [0.2]*5\n",
        "lr = 0.005\n",
        "\n",
        "def get_device(gpu=GPU):\n",
        "    return torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "def square_differncess(a,b,scalar:float=1):\n",
        "  \"\"\"\n",
        "  f,p are assumed to be in the shape of,(m,n)\n",
        "  \"\"\"\n",
        "  sub_matrix = a-b\n",
        "  square_differencess_matrix = sub_matrix*sub_matrix\n",
        "  return square_differencess_matrix.sum()*scalar\n",
        "\n",
        "def content_loss(p,x):\n",
        "  scalar = 1/2\n",
        "  return square_differncess(p,x,scalar)\n",
        "\n",
        "def element_style_loss(g,a):\n",
        "  scalar=1/(4*((g.shape[0])**2)*((a.shape[0])**2))\n",
        "  return square_differncess(g,a,scalar)\n",
        "\n",
        "def style_loss(g_seq,a_seq,weights):\n",
        "    weights = torch.tensor(weights)\n",
        "    style_loss_elements = torch.tensor(list(map(element_style_loss,g_seq,a_seq)))\n",
        "    return torch.sum(style_loss_elements*weights)\n",
        "\n",
        "class unsqueezeTransform():\n",
        "  def __init__(self,dim = 0):\n",
        "    self.dim = dim\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    return torch.unsqueeze(sample,dim=self.dim)\n",
        "\n",
        "def imshow(img, title=None):\n",
        "  if len(img.shape) > 3:\n",
        "    img = torch.squeeze(img)\n",
        "\n",
        "  plt.imshow(img)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "def early_stopping(loss_buffer:list,patience:int,minimize = True):\n",
        "  if len(loss_buffer)<=patience:\n",
        "    return False\n",
        "  \n",
        "  if minimize:\n",
        "    min_item = min(loss_buffer[-patience-1:])\n",
        "    index_of_min = loss_buffer[-patience-1:].index(min_item)\n",
        "    if index_of_min == 0:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  if not minimize:\n",
        "    max_item = max(loss_buffer[-patience-1:])\n",
        "    index_of_max = loss_buffer[-patience-1:].index(max_item)\n",
        "    if index_of_max == 0:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "# define the transformer for image preprocess.\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    unsqueezeTransform(0)\n",
        "    \n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "CVhcWYHZjxbo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_aeG2c2yVXR1"
      },
      "outputs": [],
      "source": [
        "# The following class will provide the representation of the content and the style of a given image.\n",
        "\n",
        "class ImageStyleRepresentor:\n",
        "  supported_pretrained_modles = {'vgg16':models.vgg16,'vgg19':models.vgg19}\n",
        "  def __init__(self,model_name:str = 'vgg16', device=None) -> None:\n",
        "    device= device if device is not None else torch.device('cpu') \n",
        "    self.device = device\n",
        "    pre_trained_model = self.supported_pretrained_modles[model_name](pretrained=True).to(self.device)\n",
        "    features = list(pre_trained_model.features)\n",
        "    self.features = nn.ModuleList(features).eval()\n",
        "\n",
        "  def get_content_representaion(self, img, content_layer_idx:int=0):\n",
        "    img = img.to(self.device)\n",
        "    for layer_idx,model in enumerate(self.features):\n",
        "            img = model(img)\n",
        "            if layer_idx == content_layer_idx:\n",
        "                return img\n",
        "\n",
        "  def get_style_representation(self, img, style_layers_idx:list=None):\n",
        "    representation_list = []\n",
        "    img = img.to(self.device)\n",
        "    for layer_idx,model in enumerate(self.features):\n",
        "          img = model(img)\n",
        "          if layer_idx in style_layers_idx :\n",
        "            representation_list.append(img)\n",
        "    return list(map(self._gram_multiplication_wrapper,representation_list))\n",
        "  # Should be a static function\n",
        "  def _gram_multiplication(self,t:torch.tensor):\n",
        "    # currently t is assumed to in the follwoing shape (n_filters,flatted_rep)\n",
        "        return torch.matmul(t,t.transpose(0,1))\n",
        "  # Should be a static function\n",
        "  def _gram_multiplication_wrapper(self,t:torch.tensor):\n",
        "      # t is assumed to be of the follwoing shape (1,n_filters,m_rep,m_rep)\n",
        "      if len(t.shape)>2:\n",
        "        t = t.squeeze(0)\n",
        "        t = t.flatten(start_dim=1)\n",
        "      return self._gram_multiplication(t)\n",
        "  \n",
        "\n",
        "  \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_img = Image.open(style_img_path)\n",
        "content_img = Image.open(content_img_path)\n",
        "\n",
        "preprocessed_style_img = preprocess(style_img)\n",
        "preprocessed_content_img = preprocess(content_img)\n",
        "\n",
        "\n",
        "imarray = np.random.rand(224,224,3) * 255\n",
        "transfered_img = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
        "transfered_img = preprocess(transfered_img)\n",
        "transfered_img = transfered_img.clone().detach().requires_grad_(True)\n",
        "\n",
        "isr =ImageStyleRepresentor(pre_trained_model_name,device=get_device())\n",
        "\n",
        "real_content_representation = isr.get_content_representaion(preprocessed_content_img,content_representation_layer)\n",
        "real_style_representation = isr.get_style_representation(preprocessed_style_img,style_representation_layres)\n",
        "\n",
        "\n",
        "loss_buffer = []\n",
        "optimizer = torch.optim.Adam([transfered_img],lr =lr)\n",
        "\n",
        "\n",
        "while not early_stopping(loss_buffer,patience):\n",
        "  optimizer.zero_grad()\n",
        "  transfered_img_content_representation = isr.get_content_representaion(transfered_img,content_representation_layer)\n",
        "  transfered_img_style_representation = isr.get_style_representation(transfered_img,style_representation_layres)\n",
        "\n",
        "  c_loss = content_loss(real_content_representation,transfered_img_content_representation)\n",
        "  s_loss = style_loss(real_style_representation,transfered_img_style_representation,weights)\n",
        "  total_loss = alpha*c_loss + beta*s_loss\n",
        "  print(f'this is epoch number {epoch}, the loss is {total_loss}')\n",
        "\n",
        "  \n",
        "  writer.add_scalar('content_loss',c_loss.detach(),epoch)\n",
        "  writer.add_scalar('style_loss',s_loss.detach(),epoch)\n",
        "  writer.add_scalar('total_loss',total_loss.detach(),epoch)\n",
        "\n",
        "  loss_buffer.append(total_loss.detach())\n",
        "\n",
        "  total_loss.backward(retain_graph=True)\n",
        "  optimizer.step()\n",
        "  if epoch%1000==0:\n",
        "    save_image(transfered_img.detach(), f'transfered_img{epoch}.png')\n",
        "  epoch+=1\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "save_image(transfered_img.detach(), f'transfered_img{epoch}.png')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "try:\n",
        "  writer.add_image('transfered_img',transfered_img.squeeze().transpose(0,2),epoch)\n",
        "except TypeError:\n",
        "  pass\n",
        "\"\"\"  "
      ],
      "metadata": {
        "id": "Ufv8R4knFtG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "526b5fa0-7746-45f4-80aa-d2bb79a50242"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is epoch number 0, the loss is 42046080.0\n",
            "this is epoch number 1, the loss is 41356360.0\n",
            "this is epoch number 2, the loss is 40722256.0\n",
            "this is epoch number 3, the loss is 40134196.0\n",
            "this is epoch number 4, the loss is 39578104.0\n",
            "this is epoch number 5, the loss is 39050760.0\n",
            "this is epoch number 6, the loss is 38545736.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-2f9dc5986a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mloss_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/imag_style_transfer_exp\n"
      ],
      "metadata": {
        "id": "Tz_ZFNpZnH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DhB7y3XMHb7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}