{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-Style-Transfer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNY2mzC8i8hsODdB9V5MYT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon200102/ImageStyleTransfer/blob/main/Image_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic initialization\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.utils import save_image\n",
        "# enable cuda device\n",
        "GPU = 0\n",
        "epoch = 0\n",
        "# The paths to the content and style images\n",
        "content_img_path = Path('/content/Neckarfront_TÃ¼bingen_Mai_2017.jpg')\n",
        "style_img_path = Path('/content/starry-night-1093721_960_720.jpg')\n",
        "writer = SummaryWriter('/content/imag_style_transfer_exp')\n",
        "pre_trained_model_name = 'vgg19'\n",
        "start_from_noise = False\n",
        "# parameters defined by the article\n",
        "patience = 100\n",
        "alpha =1\n",
        "beta=1000\n",
        "content_representation_layer = 21\n",
        "style_representation_layres = [0,5,10,19,28]\n",
        "weights = [0.2]*5\n",
        "lr = 0.005\n",
        "\n",
        "def get_device(gpu=GPU):\n",
        "    return torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# The basic loss fuctions\n",
        "\n",
        "def square_differncess(a,b,scalar:float=1):\n",
        "  \"\"\"\n",
        "  f,p are assumed to be in the shape of,(m,n)\n",
        "  \"\"\"\n",
        "  sub_matrix = a-b\n",
        "  square_differencess_matrix = sub_matrix*sub_matrix\n",
        "  return square_differencess_matrix.sum()*scalar\n",
        "\n",
        "def content_loss(p,x):\n",
        "  scalar = 1/2\n",
        "  return square_differncess(p,x,scalar)\n",
        "\n",
        "def element_style_loss(g,a):\n",
        "  scalar=1/(4*((g.shape[0])**2)*((a.shape[0])**2))\n",
        "  return square_differncess(g,a,scalar)\n",
        "\n",
        "def style_loss(g_seq,a_seq,weights):\n",
        "    style_loss_elements = list(map(element_style_loss,g_seq,a_seq))\n",
        "    return sum(map(lambda x,y: x*y,style_loss_elements,weights))\n",
        "\n",
        "class unsqueezeTransform():\n",
        "  def __init__(self,dim = 0):\n",
        "    self.dim = dim\n",
        "  \n",
        "  def __call__(self, sample):\n",
        "    return torch.unsqueeze(sample,dim=self.dim)\n",
        "\n",
        "def imshow(img, title=None):\n",
        "  if len(img.shape) > 3:\n",
        "    img = torch.squeeze(img)\n",
        "\n",
        "  plt.imshow(img)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "def early_stopping(loss_buffer:list,patience:int,minimize = True):\n",
        "  if len(loss_buffer)<=patience:\n",
        "    return False\n",
        "  \n",
        "  if minimize:\n",
        "    min_item = min(loss_buffer[-patience-1:])\n",
        "    index_of_min = loss_buffer[-patience-1:].index(min_item)\n",
        "    if index_of_min == 0:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  if not minimize:\n",
        "    max_item = max(loss_buffer[-patience-1:])\n",
        "    index_of_max = loss_buffer[-patience-1:].index(max_item)\n",
        "    if index_of_max == 0:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "# define the transformer for image preprocess.\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    unsqueezeTransform(0)\n",
        "    \n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "CVhcWYHZjxbo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_aeG2c2yVXR1"
      },
      "outputs": [],
      "source": [
        "# The following class will provide the representation of the content and the style of a given image.\n",
        "\n",
        "class ImageStyleRepresentor:\n",
        "  supported_pretrained_modles = {'vgg16':models.vgg16,'vgg19':models.vgg19}\n",
        "  def __init__(self,model_name:str = 'vgg16', device=None) -> None:\n",
        "    device= device if device is not None else torch.device('cpu') \n",
        "    self.device = device\n",
        "    pre_trained_model = self.supported_pretrained_modles[model_name](pretrained=True).to(self.device)\n",
        "    features = list(pre_trained_model.features)\n",
        "    self.features = nn.ModuleList(features).eval()\n",
        "\n",
        "  def get_content_representaion(self, img, content_layer_idx:int=0):\n",
        "    img = img.to(self.device)\n",
        "    for layer_idx,model in enumerate(self.features):\n",
        "            img = model(img)\n",
        "            if layer_idx == content_layer_idx:\n",
        "                return img\n",
        "\n",
        "  def get_style_representation(self, img, style_layers_idx:list=None):\n",
        "    representation_list = []\n",
        "    img = img.to(self.device)\n",
        "    for layer_idx,model in enumerate(self.features):\n",
        "          img = model(img)\n",
        "          if layer_idx in style_layers_idx :\n",
        "            representation_list.append(img)\n",
        "    return list(map(self._gram_multiplication_wrapper,representation_list))\n",
        "  # Should be a static function\n",
        "  def _gram_multiplication(self,t:torch.tensor):\n",
        "    # currently t is assumed to in the follwoing shape (n_filters,flatted_rep)\n",
        "        return torch.matmul(t,t.transpose(0,1))\n",
        "  # Should be a static function\n",
        "  def _gram_multiplication_wrapper(self,t:torch.tensor):\n",
        "      # t is assumed to be of the follwoing shape (1,n_filters,m_rep,m_rep)\n",
        "      if len(t.shape)>2:\n",
        "        t = t.squeeze(0)\n",
        "        t = t.flatten(start_dim=1)\n",
        "      return self._gram_multiplication(t)\n",
        "  \n",
        "\n",
        "  \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open and preprocess the content and style images \n",
        "style_img = Image.open(style_img_path)\n",
        "content_img = Image.open(content_img_path)\n",
        "\n",
        "preprocessed_style_img = preprocess(style_img)\n",
        "preprocessed_content_img = preprocess(content_img)\n",
        "\n",
        "if start_from_noise:\n",
        "  # Sample a random noise and preprocess it.\n",
        "  imarray = np.random.rand(224,224,3) * 255\n",
        "  transfered_img = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n",
        "  transfered_img = preprocess(transfered_img)\n",
        "  transfered_img = transfered_img.clone().detach().requires_grad_(True)\n",
        "else:\n",
        "  transfered_img = preprocessed_content_img.clone().detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "# Load the pretrained model\n",
        "isr =ImageStyleRepresentor(pre_trained_model_name,device=get_device())\n",
        "\n",
        "# \n",
        "real_content_representation = isr.get_content_representaion(preprocessed_content_img,content_representation_layer)\n",
        "real_style_representation = isr.get_style_representation(preprocessed_style_img,style_representation_layres)\n",
        "\n",
        "\n",
        "loss_buffer = []\n",
        "optimizer = torch.optim.Adam([transfered_img],lr =lr)\n",
        "\n",
        "\n",
        "while not early_stopping(loss_buffer,patience):\n",
        "  optimizer.zero_grad()\n",
        "  transfered_img_content_representation = isr.get_content_representaion(transfered_img,content_representation_layer)\n",
        "  transfered_img_style_representation = isr.get_style_representation(transfered_img,style_representation_layres)\n",
        "\n",
        "  c_loss = content_loss(real_content_representation,transfered_img_content_representation)\n",
        "  s_loss = style_loss(real_style_representation,transfered_img_style_representation,weights)\n",
        "  total_loss = alpha*c_loss + beta*s_loss\n",
        "  print(f'this is epoch number {epoch}, the loss is {total_loss}')\n",
        "\n",
        "  \n",
        "  writer.add_scalar('content_loss',c_loss.detach(),epoch)\n",
        "  writer.add_scalar('style_loss',s_loss.detach(),epoch)\n",
        "  writer.add_scalar('total_loss',total_loss.detach(),epoch)\n",
        "\n",
        "  loss_buffer.append(total_loss.detach())\n",
        "\n",
        "  total_loss.backward(retain_graph=True)\n",
        "  optimizer.step()\n",
        "  if epoch%1000==0:\n",
        "    save_image(transfered_img.detach(), f'transfered_img{epoch}.png')\n",
        "  epoch+=1\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "save_image(transfered_img.detach(), f'transfered_img{epoch}.png')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ufv8R4knFtG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/imag_style_transfer_exp\n"
      ],
      "metadata": {
        "id": "Tz_ZFNpZnH9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DhB7y3XMHb7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}